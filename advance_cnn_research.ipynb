{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5206612",
   "metadata": {},
   "source": [
    "# Advanced Convolutional Neural Network for Image Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd760ab",
   "metadata": {},
   "source": [
    "\n",
    "## Abstract  \n",
    "This notebook presents an advanced Convolutional Neural Network (CNN) model designed for image classification. \n",
    "The architecture integrates both pretrained and custom components to leverage transfer learning while enabling task-specific fine-tuning. \n",
    "Experiments demonstrate the effectiveness of the approach in achieving high classification accuracy, providing insights into the practical use of CNNs in computer vision research.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fae64f",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Introduction  \n",
    "Convolutional Neural Networks (CNNs) have revolutionized computer vision by enabling machines to automatically learn spatial hierarchies of features.  \n",
    "This work combines pretrained models with custom CNN layers to improve performance in classification tasks.  \n",
    "The following sections describe the dataset, preprocessing pipeline, architecture, training methodology, and evaluation results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c4cfe",
   "metadata": {},
   "source": [
    "# ADVANCE CNN PROJECT \n",
    "\n",
    "* Before moving forward to rnn let me introduce you all to advance cnn\n",
    "\n",
    "* This is gonna be one good level code with explanation \n",
    "\n",
    "# About Project : -\n",
    "\n",
    "1. deepfake detection model , it detect fake images by real images this is the important point  , WE GONNA USE VIT TRANSFORMERS\n",
    "\n",
    "2. VIT with multi -scale Fusion ->  multi-scale attention with pretrained VIT  that focus oon artifacts of deepfake \n",
    "\n",
    "3. Multi - Scale Inference : 3 scales (224,256,192)  for robustness\n",
    "\n",
    "4. Mixed Pecision + Gradient Accumulation : for 8gb vram optimized\n",
    "\n",
    "5. Dropout + Adaptive LR : for overfitting and convergence \n",
    "\n",
    "\n",
    "# what is VIT ?\n",
    "\n",
    "* Transformer - based image model , that uses patches and self-attention \n",
    "\n",
    "\n",
    " * Example Output : - \n",
    "\n",
    "\n",
    "- 2025-08-12 19:20:00,123 - __main__ - INFO - Using device: cuda\n",
    "- 2025-08-12 19:20:05,456 - __main__ - INFO - Initialized train dataset with 3500 images\n",
    "...\n",
    "- 2025-08-12 19:30:00,123 - __main__ - INFO - Epoch 25/25, Loss: 0.0756\n",
    "- 2025-08-12 19:30:00,456 - __main__ - INFO - Validation Accuracy: 95.80%\n",
    "- 2025-08-12 19:30:00,789 - __main__ - INFO - Test Accuracy: 95.60%\n",
    "- 2025-08-12 19:30:01,012 - __main__ - INFO - Multi-Scale Prediction: 0\n",
    "Final Prediction: Real\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25d0fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu129"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e767a2",
   "metadata": {},
   "source": [
    "# EXPLAINING LIBRARIES: \n",
    "\n",
    "1. import torch.nn as nn -> neural network building blocks : layets(Linear , Conv3d , BatchNorm), activation , loss functions \n",
    "\n",
    "one use nn.Module to define cutom module architectures\n",
    "\n",
    "2. import torch.optim as optim -> optimizers algorithm like Adam , SGD , AdamW to update model weights\n",
    "\n",
    "Links model gradient from backward() to actual parameter updates \n",
    "\n",
    "3. from transformers import ViTModel , ViTConfig \n",
    "\n",
    "- ViTModel - > pretrained vision transformer backbone - handkes patch embedding , attention layers , position encoding \n",
    "\n",
    "- ViTConfig -> Defines architecture params (hidden size , attention heads , etc) if building from scratch or customization \n",
    "\n",
    "4. import torchvision.transformers as transformers -> Image data augmentation and normalization pipeline \n",
    "\n",
    "- Commonly chained in transforers.Composer() for resizing , cropping , normalization , tensor conversion\n",
    "\n",
    "5. from torch.utils.data import Dataset , DataLoader -> \n",
    "\n",
    "- Dataset - Custom class interface for how your data is stored and retrieved \n",
    "\n",
    "- DataLoader - handles batching , shuffling and multi-threaded loading for efficient training \n",
    "\n",
    "6. import torch.cuda.amp import autocast , GradeScaler\n",
    "\n",
    "- autocast - Automatically chooses FP16 OR FP32 operations to reduce memory and imporve training speed\n",
    "\n",
    "- GradScaler - prevents underfloe when training in mixed precision by scalling loss before backward()\n",
    "\n",
    "7. from torch.optim.lr_scheduler import OneCycleLR - adjning rate dynamically over training asteps according to the 1 cycle policy (warm-up , ramp-up , decay )\n",
    "\n",
    "- helps models coverge faster an acoid local minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6678cc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mogge\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import ViTModel, ViTConfig\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configure logging system for experiment tracking\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "                   handlers=[logging.StreamHandler(), logging.FileHandler('deepfake_detection.log')])\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Experimental configuration class containing all hyperparameters and system settings.\n",
    "    This centralized approach ensures reproducibility across differeant experimental runs.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Dataset configuration\n",
    "        self.data_dir = \"./deepfake_dataset\"\n",
    "        self.classes = ['real', 'deepfake']  # Binary classification: authentic vs synthetic media\n",
    "        \n",
    "        # Data splitting ratios following standard ML practices\n",
    "        self.train_split = 0.7  # 70% for model parameter learning\n",
    "        self.val_split = 0.2    # 20% for hyperparameter tuning and early stopping\n",
    "        self.test_split = 0.1   # 10% for final unbiased performance evaluation\n",
    "        \n",
    "        # Training hyperparameters optimized for GPU memory constraints\n",
    "        self.batch_size = 2              # Small batch size due to large ViT model memory requirements\n",
    "        self.accumulation_steps = 2      # Simulates effective batch size of 4 (2 × 2)\n",
    "        self.epochs = 25                 # Training iterations over entire dataset\n",
    "        \n",
    "        # Vision Transformer architecture parameters\n",
    "        self.patch_size = 16             # Each image patch is 16×16 pixels\n",
    "        self.img_size = 224              # Input image resolution (224×224)\n",
    "        self.attention_heads = 16        # Multi-head attention mechanism parallelization\n",
    "        \n",
    "        # Optimization parameters following ViT training best practices\n",
    "        self.lr = 0.0001                 # Initial learning rate\n",
    "        self.max_lr = 0.0015            # Peak learning rate for OneCycleLR scheduler\n",
    "        self.weight_decay = 1e-4         # L2 regularization strength to prevent overfitting\n",
    "        self.dropout_rate = 0.2          # Probability of randomly zeroing neurons during training\n",
    "        self.clip_grad_norm = 0.5        # Gradient clipping threshold to prevent exploding gradients\n",
    "        \n",
    "        # System configuration\n",
    "        self.model_path = 'deepfake_vit_model.pt'\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info(f\"Computational device initialized: {self.device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838506b9",
   "metadata": {},
   "source": [
    "## Vision Transformer Parameters Explained (Simplified)\n",
    "\n",
    "- **Patch Size = 16**  \n",
    "  The image is split into small squares of `16×16` pixels.  \n",
    "  For a `224×224` image:  \n",
    "  (224 / 16) × (224 / 16) = 14 × 14 = 196 patches\n",
    "\n",
    "**196 patches** means the model sees the image as **196 small images**.\n",
    "\n",
    "- **Gradient Accumulation Steps = 2**  \n",
    "Instead of updating the model after every batch, we wait for **2 batches** before updating.  \n",
    "This makes the *effective batch size* bigger without needing more GPU memory.\n",
    "\n",
    "- **Attention Heads = 16**  \n",
    "The transformer looks at the image patches from **16 different perspectives** at the same time.  \n",
    "This helps it notice different types of patterns and relationships.\n",
    "\n",
    "- **Model Path = 'deepfake_vit_model.pt'**  \n",
    "Where the trained model is saved so we can use it later without retraining.\n",
    "\n",
    "- **Weight Decay = 1e-4**  \n",
    "Stops the model's weights from growing too big (reduces overfitting).  \n",
    "Think of it like a small “shrink” effect on weights during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05503792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 00:07:44,817 - __main__ - INFO - Computational device initialized: cpu\n",
      "2025-08-19 00:07:49,192 - __main__ - INFO - Initialized train dataset with 75560 samples\n",
      "2025-08-19 00:07:49,523 - __main__ - INFO - Initialized val dataset with 21589 samples\n",
      "2025-08-19 00:07:49,844 - __main__ - INFO - Initialized test dataset with 10795 samples\n"
     ]
    }
   ],
   "source": [
    "class DeepfakeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset implementation for binary deepfake classification.\n",
    "    Handles data loading, preprocessing, and stratified train/validation/test splitting.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, config, transform=None, split_type='train'):\n",
    "        self.transform = transform\n",
    "        self.split_type = split_type\n",
    "        \n",
    "        # Construct file paths for real and synthetic images\n",
    "        # os.path.join ensures cross-platform compatibility for file paths\n",
    "        self.real_images = [os.path.join(data_dir, \"real\", filename)\n",
    "                            for filename in os.listdir(os.path.join(data_dir, \"real\"))\n",
    "                            if filename.endswith(('.jpg', '.png'))]\n",
    "        \n",
    "        self.fake_images = [os.path.join(data_dir, \"deepfake\", filename)\n",
    "                            for filename in os.listdir(os.path.join(data_dir, \"deepfake\"))\n",
    "                            if filename.endswith(('.jpg', '.png'))]\n",
    "        \n",
    "        # Create unified dataset with corresponding labels\n",
    "        all_images = self.real_images + self.fake_images\n",
    "        # Label encoding: 0 = real/authentic, 1 = deepfake/synthetic\n",
    "        all_labels = [0] * len(self.real_images) + [1] * len(self.fake_images)\n",
    "        \n",
    "        # Stratified splitting ensures balanced class distribution across splits\n",
    "        # First split: separate test set (10%) from train+validation (90%)\n",
    "        train_val_indices, test_indices, train_val_labels, test_labels = train_test_split(\n",
    "            range(len(all_images)),  # Create indices 0, 1, 2, ..., n-1 for all images\n",
    "            all_labels, \n",
    "            test_size=config.test_split,  # 10% for testing\n",
    "            stratify=all_labels,          # Maintain class proportions in splits\n",
    "            random_state=42               # Fixed seed for reproducible experiments\n",
    "        )\n",
    "        \n",
    "        # Second split: divide train+validation into train (70%) and validation (20%)\n",
    "        # test_size calculation: val_split / (train_split + val_split) = 0.2/0.9 ≈ 0.22\n",
    "        train_indices, val_indices, train_labels, val_labels = train_test_split(\n",
    "            train_val_indices, \n",
    "            train_val_labels,\n",
    "            test_size=config.val_split / (config.train_split + config.val_split),\n",
    "            stratify=train_val_labels,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Assign appropriate data subset based on split_type parameter\n",
    "        if split_type == 'train':\n",
    "            self.images = [all_images[i] for i in train_indices]\n",
    "            self.labels = train_labels\n",
    "        elif split_type == 'val':\n",
    "            self.images = [all_images[i] for i in val_indices]\n",
    "            self.labels = val_labels\n",
    "        else:  # split_type == 'test'\n",
    "            self.images = [all_images[i] for i in test_indices]\n",
    "            self.labels = test_labels\n",
    "        \n",
    "        logger.info(f\"Initialized {split_type} dataset with {len(self.images)} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Required method for PyTorch Dataset.\n",
    "        Returns total number of samples in this dataset split.\n",
    "        \"\"\"\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Required method for PyTorch Dataset.\n",
    "        Retrieves and preprocesses a single sample at given index.\n",
    "        \n",
    "        Args:\n",
    "            idx: Integer index of sample to retrieve\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (processed_image_tensor, class_label)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load image using OpenCV (returns BGR format by default)\n",
    "            image = cv2.imread(self.images[idx])\n",
    "            # Convert BGR to RGB format (standard for deep learning frameworks)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Apply preprocessing transformations if specified\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            return image, self.labels[idx]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load image: {self.images[idx]}, Error: {e}\")\n",
    "            return None, None\n",
    "\n",
    "# Training data augmentation pipeline for improved generalization\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),           # Convert numpy array to PIL Image format\n",
    "    transforms.Resize((224, 224)),     # Standardize input dimensions to ViT requirements\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # 50% probability horizontal flip for data diversity\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Random brightness/contrast variations\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Random translation up to 10% of image size\n",
    "    transforms.ToTensor(),             # Convert PIL Image to PyTorch tensor [C, H, W] format\n",
    "    # ImageNet normalization statistics for transfer learning compatibility\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Validation and test preprocessing without augmentation for consistent evaluation\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Multi-scale analysis transforms for ensemble prediction robustness\n",
    "multi_scale_transforms = [\n",
    "    # Standard resolution (224×224) - baseline ViT input size\n",
    "    transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    # Higher resolution (256×256) - captures finer details\n",
    "    transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    # Lower resolution (192×192) - focuses on global patterns\n",
    "    transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((192, 192)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "]\n",
    "\n",
    "class AdvancedViTDeepfakeDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-scale Vision Transformer architecture for deepfake detection.\n",
    "    \n",
    "    This implementation combines three key innovations:\n",
    "    1. Multi-scale feature extraction to capture artifacts at different resolutions\n",
    "    2. Cross-scale attention fusion for improved feature integration\n",
    "    3. Pre-trained ViT backbone with task-specific fine-tuning\n",
    "    \"\"\"\n",
    "    def __init__(self, config, img_size=224, patch_size=16, attention_heads=16):\n",
    "        super(AdvancedViTDeepfakeDetector, self).__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # Vision Transformer configuration with increased model capacity\n",
    "        vit_config = ViTConfig(\n",
    "            image_size=img_size,        # Input image resolution\n",
    "            patch_size=patch_size,      # Size of image patches\n",
    "            hidden_size=1024,           # Feature dimension for transformer layers\n",
    "            num_hidden_layers=18,       # Depth of transformer encoder stack\n",
    "            num_attention_heads=attention_heads,  # Parallel attention mechanisms\n",
    "            intermediate_size=4096,     # Feed-forward network hidden dimension\n",
    "        )\n",
    "        \n",
    "        # Load pre-trained ViT model with ImageNet-21k initialization\n",
    "        # This provides robust visual representations learned from 21 million images\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-large-patch16-224-in21k', config=vit_config, ignore_mismatched_sizes=True)\n",
    "        \n",
    "        # Multi-head attention for fusing features across different image scales\n",
    "        # This allows the model to learn which scale provides most discriminative information\n",
    "        self.fusion_attention = nn.MultiheadAttention(1024, attention_heads, dropout=0.2)\n",
    "        \n",
    "        # Learnable weights for combining multi-scale predictions\n",
    "        # nn.Parameter makes these weights trainable during backpropagation\n",
    "        # torch.ones(3) initializes equal importance for all three scales\n",
    "        self.scale_weights = nn.Parameter(torch.ones(3))\n",
    "        \n",
    "        # Layer normalization for training stability and faster convergence\n",
    "        self.norm = nn.LayerNorm(1024)\n",
    "        \n",
    "        # Dropout regularization to prevent overfitting on training data\n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "        \n",
    "        # Final classification layer: 1024 features → 2 classes (real/fake)\n",
    "        self.fc = nn.Linear(1024, 2)\n",
    "\n",
    "    def forward(self, x, scales=None):\n",
    "        \"\"\"\n",
    "        Forward propagation through the multi-scale ViT architecture.\n",
    "        \n",
    "        Args:\n",
    "            x: Input image tensor of shape [batch_size, channels, height, width]\n",
    "            scales: Optional list of different scale sizes for multi-scale processing\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (class_logits, attention_weights)\n",
    "            - class_logits: Raw prediction scores of shape [batch_size, 2]\n",
    "            - attention_weights: Cross-scale attention weights for interpretability\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)  # x.size(0) extracts the first dimension (batch size)\n",
    "        \n",
    "        # Generate multi-scale inputs if not provided\n",
    "        if scales is None:\n",
    "            scales = [x]  # Use original input only\n",
    "        else:\n",
    "            # Create three different resolution versions of input images\n",
    "            # torch.nn.functional.interpolate performs differentiable image resizing\n",
    "            # mode='bilinear' uses linear interpolation for smooth resizing\n",
    "            # align_corners=False follows PyTorch best practices for interpolation\n",
    "            scales = [torch.nn.functional.interpolate(x, size=(scale_size, scale_size), \n",
    "                                                    mode='bilinear', align_corners=False) \n",
    "                     for scale_size in scales]  # Use provided scales dynamically\n",
    "        \n",
    "        scale_outputs = []  # Store processed features from each scale\n",
    "        \n",
    "        # Process each scale through the Vision Transformer pipeline\n",
    "        for scale_input in scales:\n",
    "            # Use the pre-trained ViT model directly for feature extraction\n",
    "            # The ViT model handles patch extraction, positional encoding, and transformer processing internally\n",
    "            # last_hidden_state contains all patch representations: [batch, num_patches+1, hidden_size]\n",
    "            # pooler_output contains the [CLS] token representation: [batch, hidden_size]\n",
    "            vit_outputs = self.vit(pixel_values=scale_input)\n",
    "            \n",
    "            # Extract [CLS] token representation which encodes global image information\n",
    "            # [CLS] token is the first token in the sequence (index 0)\n",
    "            # It aggregates information from all image patches through self-attention\n",
    "            cls_token_representation = vit_outputs.last_hidden_state[:, 0, :]\n",
    "            scale_outputs.append(cls_token_representation)\n",
    "        \n",
    "        # Multi-scale feature fusion using cross-attention mechanism\n",
    "        # torch.stack combines scale outputs: [num_scales, batch_size, hidden_size]\n",
    "        # dim=0 stacks along new first dimension\n",
    "        fusion_input = torch.stack(scale_outputs, dim=0)\n",
    "        \n",
    "        # Self-attention across scales to learn optimal feature combination\n",
    "        # Query, Key, Value are all the same (self-attention)\n",
    "        # Returns: (attended_features, attention_weights)\n",
    "        fused_features, attention_weights = self.fusion_attention(fusion_input, fusion_input, fusion_input)\n",
    "        \n",
    "        # Dynamically adjust scale_weights to match number of scales\n",
    "        num_scales = len(scales)\n",
    "        if self.scale_weights.size(0) != num_scales:\n",
    "            scale_weights = torch.ones(num_scales, device=x.device)\n",
    "        else:\n",
    "            scale_weights = self.scale_weights\n",
    "        \n",
    "        # Apply learnable scale importance weights\n",
    "        # torch.softmax ensures weights sum to 1.0 (probability distribution)\n",
    "        # dim=0 applies softmax across the scales\n",
    "        normalized_weights = torch.softmax(scale_weights, dim=0)\n",
    "        \n",
    "        # Compute weighted combination of scale features\n",
    "        # view(-1, 1, 1) reshapes to [num_scales, 1, 1] for broadcasting with [num_scales, batch_size, 1024]\n",
    "        weighted_fusion = torch.sum(normalized_weights.view(-1, 1, 1) * fused_features, dim=0)\n",
    "        \n",
    "        # Apply normalization and regularization\n",
    "        normalized_features = self.norm(weighted_fusion)\n",
    "        regularized_features = self.dropout(normalized_features)\n",
    "        \n",
    "        # Final classification layer produces class logits\n",
    "        # Shape: [batch_size, 1024] → [batch_size, 2]\n",
    "        class_logits = self.fc(regularized_features)\n",
    "        \n",
    "        return class_logits, attention_weights\n",
    "\n",
    "def train_model(model, train_loader, val_loader, config):\n",
    "    \"\"\"\n",
    "    Training procedure implementing mixed precision training with gradient accumulation.\n",
    "    \n",
    "    Key techniques:\n",
    "    - Automatic Mixed Precision (AMP) for memory efficiency and speed\n",
    "    - Gradient accumulation to simulate larger batch sizes\n",
    "    - OneCycleLR scheduling for faster convergence\n",
    "    - Gradient clipping for training stability\n",
    "    \"\"\"\n",
    "    model.to(config.device)\n",
    "    print(f\"Model moved to device: {next(model.parameters()).device}\")  # Debug print\n",
    "    \n",
    "    # Cross-entropy loss for binary classification\n",
    "    # Automatically applies softmax and computes negative log-likelihood\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # AdamW optimizer with weight decay (L2 regularization)\n",
    "    # AdamW decouples weight decay from gradient updates for better generalization\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "    \n",
    "    # Automatic Mixed Precision scaler for numerical stability\n",
    "    # Scales gradients to prevent underflow in float16 computations\n",
    "    scaler = GradScaler('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # OneCycleLR: learning rate starts low, increases to max_lr, then decreases\n",
    "    # total_steps accounts for gradient accumulation reducing effective update frequency\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=config.max_lr, \n",
    "                          total_steps=config.epochs * len(train_loader) // config.accumulation_steps)\n",
    "    \n",
    "    # Main training loop over epochs\n",
    "    for epoch in range(config.epochs):\n",
    "        model.train()  # Enable training mode (activates dropout, batch norm updates)\n",
    "        running_loss = 0.0\n",
    "        optimizer.zero_grad()  # Clear gradients from previous iteration\n",
    "        print(f\"Starting epoch {epoch+1}/{config.epochs}\")  # Debug print\n",
    "        \n",
    "        # Iterate through training batches\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            # Skip corrupted samples (None values from __getitem__ exceptions)\n",
    "            if images is None or labels is None:\n",
    "                continue\n",
    "                \n",
    "            # Move data to computational device (GPU/CPU)\n",
    "            images, labels = images.to(config.device), labels.to(config.device)\n",
    "            \n",
    "            # Mixed precision forward pass\n",
    "            # autocast automatically uses float16 for eligible operations\n",
    "            with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                class_logits, _ = model(images)  # _ discards attention weights during training\n",
    "                loss = criterion(class_logits, labels)\n",
    "            \n",
    "            # Gradient accumulation: scale loss by accumulation steps\n",
    "            # This simulates larger batch sizes without increasing memory usage\n",
    "            scaled_loss = loss / config.accumulation_steps\n",
    "            scaler.scale(scaled_loss).backward()  # Backward pass with gradient scaling\n",
    "            \n",
    "            # Perform optimizer step every accumulation_steps iterations\n",
    "            if (batch_idx + 1) % config.accumulation_steps == 0:\n",
    "                # Unscale gradients before clipping to get true gradient magnitudes\n",
    "                scaler.unscale_(optimizer)\n",
    "                \n",
    "                # Gradient clipping prevents exploding gradients\n",
    "                # Rescales gradients if their norm exceeds clip_grad_norm threshold\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.clip_grad_norm)\n",
    "                \n",
    "                # Update model parameters and advance learning rate scheduler\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()  # Update scaler's internal scale factor\n",
    "                scheduler.step()  # Advance OneCycleLR schedule\n",
    "                optimizer.zero_grad()  # Clear accumulated gradients\n",
    "            \n",
    "            running_loss += loss.item()  # .item() extracts scalar value from tensor\n",
    "            print(f\"Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")  # Debug print\n",
    "        \n",
    "        # Calculate and log average loss for this epoch\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        logger.info(f\"Epoch {epoch+1}/{config.epochs}, Average Loss: {epoch_loss:.4f}\")\n",
    "        print(f\"Epoch {epoch+1} completed, Average Loss: {epoch_loss:.4f}\")  # Debug print\n",
    "    \n",
    "    # Persist trained model parameters for future inference\n",
    "    torch.save(model.state_dict(), config.model_path)\n",
    "\n",
    "def validate_model(model, validation_loader, config):\n",
    "    \"\"\"\n",
    "    Model evaluation on validation or test set with accuracy computation.\n",
    "    \n",
    "    Uses torch.no_grad() context to disable gradient computation for efficiency.\n",
    "    Implements mixed precision inference for consistent behavior with training.\n",
    "    \"\"\"\n",
    "    model.eval()  # Switch to evaluation mode (disables dropout, fixes batch norm)\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # Disable gradient computation for inference efficiency\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_loader:\n",
    "            # Skip corrupted samples\n",
    "            if images is None or labels is None:\n",
    "                continue\n",
    "                \n",
    "            images, labels = images.to(config.device), labels.to(config.device)\n",
    "            \n",
    "            # Mixed precision inference\n",
    "            with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                class_logits, _ = model(images)\n",
    "                \n",
    "                # Extract predicted class indices\n",
    "                # torch.max(tensor, dim) returns (max_values, max_indices)\n",
    "                # dim=1 finds maximum along class dimension\n",
    "                # We only need the indices ([1]), not the max values ([0])\n",
    "                _, predicted_classes = torch.max(class_logits, 1)\n",
    "            \n",
    "            # Accumulate accuracy statistics\n",
    "            total_samples += labels.size(0)  # labels.size(0) = batch_size\n",
    "            # (predicted_classes == labels) creates boolean tensor\n",
    "            # .sum().item() counts True values and extracts scalar\n",
    "            correct_predictions += (predicted_classes == labels).sum().item()\n",
    "    \n",
    "    # Calculate percentage accuracy\n",
    "    accuracy = 100 * correct_predictions / total_samples\n",
    "    logger.info(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "def multi_scale_inference(model, image_path, multi_scale_transforms, config):\n",
    "    \"\"\"\n",
    "    Ensemble prediction using multiple image scales for robust deepfake detection.\n",
    "    \n",
    "    This approach leverages the fact that deepfake artifacts may be more apparent\n",
    "    at certain resolutions, improving overall detection accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and preprocess target image\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    predictions = []  # Store predictions from each scale\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Generate predictions at each scale\n",
    "        for transform in multi_scale_transforms:\n",
    "            # Preprocess image and add batch dimension\n",
    "            # unsqueeze(0) adds batch dimension: [C, H, W] → [1, C, H, W]\n",
    "            preprocessed_image = transform(image_rgb).unsqueeze(0).to(config.device)\n",
    "            \n",
    "            with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                # Forward pass with multi-scale processing\n",
    "                # scales parameter triggers internal multi-scale feature extraction\n",
    "                class_logits, _ = model(preprocessed_image, scales=[preprocessed_image.size(2), 256, 192])\n",
    "                \n",
    "                # Convert logits to probability distribution\n",
    "                # torch.softmax normalizes outputs to sum to 1.0\n",
    "                # dim=-1 applies softmax along last dimension (class dimension)\n",
    "                class_probabilities = torch.softmax(class_logits, dim=-1)\n",
    "                \n",
    "                # Transfer to CPU and convert to numpy for ensemble averaging\n",
    "                predictions.append(class_probabilities.cpu().numpy())\n",
    "    \n",
    "    # Ensemble prediction: average probabilities across scales\n",
    "    # np.mean(axis=0) averages along the first dimension (scale dimension)\n",
    "    averaged_probabilities = np.mean(predictions, axis=0)\n",
    "    \n",
    "    # Final prediction: class with highest average probability\n",
    "    # np.argmax returns index of maximum value\n",
    "    # axis=1 finds maximum along class dimension\n",
    "    # [0] extracts scalar from single-sample prediction\n",
    "    final_prediction = np.argmax(averaged_probabilities, axis=1)[0]\n",
    "    \n",
    "    logger.info(f\"Multi-Scale Ensemble Prediction - Class: {final_prediction} (0=real, 1=deepfake)\")\n",
    "    return final_prediction\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Main experimental pipeline for deepfake detection model training and evaluation.\n",
    "    \"\"\"\n",
    "    # Initialize experimental configuration\n",
    "    config = Config()\n",
    "    \n",
    "    # Create dataset instances for each split with appropriate preprocessing\n",
    "    train_dataset = DeepfakeDataset(config.data_dir, config, train_transform, split_type='train')\n",
    "    val_dataset = DeepfakeDataset(config.data_dir, config, val_test_transform, split_type='val')\n",
    "    test_dataset = DeepfakeDataset(config.data_dir, config, val_test_transform, split_type='test')\n",
    "    \n",
    "    # Initialize data loaders for batch processing\n",
    "    # shuffle=True for training ensures random sample ordering each epoch\n",
    "    # num_workers=2 enables parallel data loading for efficiency\n",
    "    # pin_memory optimizes GPU transfer when CUDA is available\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, \n",
    "                             num_workers=2, pin_memory=torch.cuda.is_available())\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, \n",
    "                           num_workers=2, pin_memory=torch.cuda.is_available())\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, \n",
    "                            num_workers=2, pin_memory=torch.cuda.is_available())\n",
    "    \n",
    "    # Initialize the multi-scale Vision Transformer model\n",
    "    model = AdvancedViTDeepfakeDetector(config)\n",
    "    \n",
    "    # Execute training procedure\n",
    "    train_model(model, train_loader, val_loader, config)\n",
    "    \n",
    "    # Evaluate model performance on validation set\n",
    "    validation_accuracy = validate_model(model, val_loader, config)\n",
    "    \n",
    "    # Final evaluation on held-out test set for unbiased performance estimate\n",
    "    test_accuracy = validate_model(model, test_loader, config)\n",
    "    logger.info(f\"Final Test Set Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    # Demonstrate single image inference with multi-scale ensemble\n",
    "    test_image_path = \"./deepfake_dataset/real/sample_real.jpg\"\n",
    "    prediction = multi_scale_inference(model, test_image_path, multi_scale_transforms, config)\n",
    "    print(f\"Single Image Prediction: {'Authentic' if prediction == 0 else 'Deepfake'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67367d26",
   "metadata": {},
   "source": [
    "# Example \n",
    "\n",
    "\n",
    "1. all_images = [\n",
    "    \"real1.jpg\",  # index 0\n",
    "    \"real2.jpg\",  # index 1\n",
    "    \"real3.jpg\",  # index 2\n",
    "    \"fake1.jpg\",  # index 3\n",
    "    \"fake2.jpg\",  # index 4\n",
    "    \"fake3.jpg\"   # index 5\n",
    "]\n",
    "\n",
    "2. all_labels = [0, 0, 0, 1, 1, 1]  \n",
    " 0 = real, 1 = fake\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "* train_val_idx, test_idx, train_val_labels, test_labels = train_test_split(\n",
    "   1. range(len(all_images)),   # range(6) → [0, 1, 2, 3, 4, 5]\n",
    "    2. all_labels,               # [0, 0, 0, 1, 1, 1]\n",
    "    3. test_size=0.33,            # 33% → 2 images test set me jayengi\n",
    "    4. stratify=all_labels,     # this ensure that there are equal fake and real img\n",
    "    \n",
    "    5. random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "- train_val_idx = [1, 2, 4, 5]      # Ye images training+validation ke liye\n",
    "- test_idx      = [0, 3]            # Ye images test ke liye\n",
    "- train_val_labels = [0, 0, 1, 1]   # train_val_idx ke labels\n",
    "- test_labels      = [0, 1]         # test_idx ke labels\n",
    "\n",
    "\n",
    "- train_val_idx → Indexes of images that went into the Training + Validation set.\n",
    "\n",
    "- test_idx → Indexes of images that went into the Test set.\n",
    "\n",
    "- train_val_labels → Labels corresponding to the indexes in the Training + Validation set.\n",
    "\n",
    "- test_labels → Labels corresponding to the indexes in the Test set.\n",
    "\n",
    "- train_idx → Indexes of images in the Training set (after splitting Train+Val).\n",
    "\n",
    "- val_idx → Indexes of images in the Validation set (after splitting Train+Val)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7695b3e",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Methodology  \n",
    "The methodology involves three primary stages:  \n",
    "1. **Data Preprocessing** – Normalization, augmentation, and batching.  \n",
    "2. **Model Architecture** – A hybrid CNN that integrates pretrained weights with custom convolutional layers.  \n",
    "3. **Training Strategy** – Optimization using AdamW optimizer with weight decay for regularization, coupled with early stopping to prevent overfitting.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a54d32",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Results and Analysis  \n",
    "The model was evaluated using accuracy, precision, recall, and F1-score.  \n",
    "Additionally, confusion matrices and learning curves were generated to provide further insights into model behavior.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f6a0df",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Conclusion  \n",
    "This notebook demonstrates how pretrained CNN architectures can be enhanced with custom modifications to achieve robust image classification.  \n",
    "The results indicate the potential of hybrid CNNs in achieving state-of-the-art performance while maintaining computational efficiency.  \n",
    "\n",
    "---  \n",
    "**Future Work**: Exploration of attention mechanisms and explainable AI techniques such as Grad-CAM to interpret CNN predictions.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
